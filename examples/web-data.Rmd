---
title: "Web data"
author: "Joe Marlo"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Downloading

Motivating example: https://ftp.rma.usda.gov/pub/references/adm_livestock/

```{r}
# example of downloading a csv
lalonde <- readr::read_csv("")
head(lalonde)
```

### Handling generic files and zips

```{r}
# example of downloading one zip file
url_file <- 'https://ftp.rma.usda.gov/pub/references/adm_livestock/2014/2014_ADMLivestockLgm_Daily_20130726.zip'
destination_folder <- 'data'
dir.create(destination_folder)
path_to_zip <- file.path(destination_folder, 'temp_data.zip')
download.file(url_file, destfile = path_to_zip)
unzip(path_to_zip, exdir = destination_folder)
file.remove(path_to_zip)
```

```{r}
# turning it into a function
download_and_unzip <- function(url_file, destination_folder = 'data'){
  dir.create(destination_folder)
  path_to_zip <- file.path(destination_folder, 'temp_data.zip')
  download.file(url_file, destfile = path_to_zip)
  unzip(path_to_zip, exdir = destination_folder)
  file.remove(path_to_zip)
}

download_and_unzip('https://ftp.rma.usda.gov/pub/references/adm_livestock/2014/2014_ADMLivestockLgm_Daily_20130726.zip')
```

### Downloading an entire directory



#### The DOM

```{r}
library(dplyr)
library(rvest)

extract_file_urls <- function(url_page){
  # get the page's html
  page <- read_html(url_page)

  # find the links
  # first, examine the webpage using developer tools within your browser
  # to determine what selectors to use
  # explain DOM
  url_files <- page %>%
    html_elements('a') %>% # this selects all "a" html elements
    html_attr('href') # this extracts the html attribute "href"
  
  # make sure all links are zip files
  url_zips <- stringr::str_subset(url_files, "zip$")
  
  # add base url
  url_zips <- glue::glue('https://ftp.rma.usda.gov/{url_zips}')
  
  return(url_zips)
}

read_year_and_download <- function(year){
  # construct url and find the files
  base_url <- glue::glue('https://ftp.rma.usda.gov/pub/references/adm_livestock/{year}/')
  found_files <- extract_file_urls(base_url)
  
  # download the files
  purrr::walk(found_files, download_and_unzip)
}

# download and unzip all the files in all the years
purrr::walk(2014:2022, read_year_and_download)
```

## Scraping

How to scrape a table from wikipedia



# TODO

## Error handling

## Terms of use -- robots.txt

## CSS selectors overview

